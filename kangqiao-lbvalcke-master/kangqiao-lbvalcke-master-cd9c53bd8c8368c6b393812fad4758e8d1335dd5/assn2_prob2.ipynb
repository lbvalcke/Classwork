{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering comparison code\n",
      "50 most similar pairs of speeches by different presidents\n",
      "0.724299870882: William J. Clinton 1995 Barack Obama 2010\n",
      "0.71948479195: William J. Clinton 1995 Barack Obama 2011\n",
      "0.710569236827: William J. Clinton 1995 Barack Obama 2013\n",
      "0.709412531729: William J. Clinton 1995 Barack Obama 2012\n",
      "0.689458310524: George Bush 1992 William J. Clinton 1995\n",
      "0.67022282756: William J. Clinton 1994 Barack Obama 2010\n",
      "0.663996085883: George Bush 1992 William J. Clinton 1994\n",
      "0.662321665159: William J. Clinton 1994 Barack Obama 2011\n",
      "0.66162674439: William J. Clinton 1994 Barack Obama 2013\n",
      "0.658209724277: George Bush 1992 Barack Obama 2011\n",
      "0.65603011247: William J. Clinton 1994 Barack Obama 2012\n",
      "0.651400633829: William J. Clinton 1993 Barack Obama 2011\n",
      "0.649608139049: William J. Clinton 1995 Barack Obama 2009\n",
      "0.648968376217: William J. Clinton 1998 Barack Obama 2011\n",
      "0.64780599563: George Bush 1992 Barack Obama 2010\n",
      "0.63984576068: William J. Clinton 1994 Barack Obama 2009\n",
      "0.635137836395: William J. Clinton 1993 Barack Obama 2010\n",
      "0.626056425751: Chester A. Arthur 1883 William Howard Taft 1910\n",
      "0.625127908754: George Bush 1992 Barack Obama 2009\n",
      "0.621636037189: Dwight D. Eisenhower 1956 John F. Kennedy 1961\n",
      "0.621636037189: Dwight D. Eisenhower 1956 John F. Kennedy 1961\n",
      "0.620901983043: John Tyler 1844 James K. Polk 1846\n",
      "0.616268199566: George Bush 1992 Barack Obama 2012\n",
      "0.615486609472: Ronald Reagan 1986 George Bush 1989\n",
      "0.613770248382: William J. Clinton 1998 Barack Obama 2010\n",
      "0.612158810109: George Bush 1992 Barack Obama 2013\n",
      "0.610963789832: William J. Clinton 1993 Barack Obama 2009\n",
      "0.60889959784: Ronald Reagan 1988 George Bush 1989\n",
      "0.608145107717: George Bush 1992 William J. Clinton 1993\n",
      "0.6080389662: Grover Cleveland 1885 Benjamin Harrison 1889\n",
      "0.606907095137: William J. Clinton 1998 Barack Obama 2009\n",
      "0.605305283054: Dwight D. Eisenhower 1955 John F. Kennedy 1961\n",
      "0.605305283054: Dwight D. Eisenhower 1955 John F. Kennedy 1961\n",
      "0.604252912421: George Bush 1989 Barack Obama 2011\n",
      "0.603874685783: George Bush 1989 William J. Clinton 1998\n",
      "0.601385660837: George Bush 1989 William J. Clinton 2000\n",
      "0.599789174716: John F. Kennedy 1961 Jimmy Carter 1981\n",
      "0.599789174716: John F. Kennedy 1961 Jimmy Carter 1981\n",
      "0.599789174716: John F. Kennedy 1961 Jimmy Carter 1981\n",
      "0.599789174716: John F. Kennedy 1961 Jimmy Carter 1981\n",
      "0.595548474553: Ronald Reagan 1987 William J. Clinton 1995\n",
      "0.593861764345: Ronald Reagan 1985 Barack Obama 2011\n",
      "0.592447819585: Ronald Reagan 1987 George Bush 1992\n",
      "0.590951001932: William J. Clinton 1993 Barack Obama 2013\n",
      "0.589363283315: Ronald Reagan 1987 Barack Obama 2011\n",
      "0.589059237427: William J. Clinton 2000 Barack Obama 2011\n",
      "0.587881714291: William J. Clinton 1993 Barack Obama 2012\n",
      "0.586721880661: Andrew Jackson 1836 Martin Van Buren 1839\n",
      "0.586337792712: Rutherford B. Hayes 1877 Grover Cleveland 1885\n",
      "0.585816349814: George Bush 1992 William J. Clinton 2000\n",
      "\n",
      "50 most similar pairs of speeches by the same presidents\n",
      "0.996312277152: Barack Obama 2012 Barack Obama 2013\n",
      "0.827142330749: George W. Bush 2007 George W. Bush 2008\n",
      "0.810966256646: Barack Obama 2010 Barack Obama 2012\n",
      "0.80559902838: Barack Obama 2010 Barack Obama 2013\n",
      "0.799844114081: Barack Obama 2010 Barack Obama 2011\n",
      "0.795782761825: Barack Obama 2011 Barack Obama 2012\n",
      "0.792844340033: Barack Obama 2011 Barack Obama 2013\n",
      "0.765553626854: William J. Clinton 1994 William J. Clinton 1995\n",
      "0.756443253991: Barack Obama 2009 Barack Obama 2010\n",
      "0.747005728452: William J. Clinton 1998 William J. Clinton 1999\n",
      "0.740339026279: William J. Clinton 1998 William J. Clinton 2000\n",
      "0.739636660635: William J. Clinton 1997 William J. Clinton 1998\n",
      "0.733203575997: Lyndon B. Johnson 1966 Lyndon B. Johnson 1967\n",
      "0.724753650056: William J. Clinton 1997 William J. Clinton 1999\n",
      "0.721142307256: George W. Bush 2005 George W. Bush 2008\n",
      "0.717691512998: George W. Bush 2004 George W. Bush 2007\n",
      "0.715652576952: William J. Clinton 1999 William J. Clinton 2000\n",
      "0.713477713372: Barack Obama 2009 Barack Obama 2012\n",
      "0.713108453524: Barack Obama 2009 Barack Obama 2013\n",
      "0.706156480498: George W. Bush 2005 George W. Bush 2007\n",
      "0.703899211707: William McKinley 1899 William McKinley 1900\n",
      "0.699664094639: Barack Obama 2009 Barack Obama 2011\n",
      "0.691502041158: George W. Bush 2004 George W. Bush 2008\n",
      "0.687191151854: William J. Clinton 1993 William J. Clinton 1995\n",
      "0.683507906639: Dwight D. Eisenhower 1955 Dwight D. Eisenhower 1956\n",
      "0.678515101371: Grover Cleveland 1893 Grover Cleveland 1894\n",
      "0.678497401053: George W. Bush 2003 George W. Bush 2004\n",
      "0.675950198755: George W. Bush 2003 George W. Bush 2007\n",
      "0.675879323866: Chester A. Arthur 1881 Chester A. Arthur 1882\n",
      "0.675250063388: Chester A. Arthur 1882 Chester A. Arthur 1883\n",
      "0.673505533433: George W. Bush 2004 George W. Bush 2005\n",
      "0.673394714072: William J. Clinton 1995 William J. Clinton 1998\n",
      "0.673268729609: George W. Bush 2006 George W. Bush 2007\n",
      "0.670458314022: William J. Clinton 1993 William J. Clinton 1994\n",
      "0.663670978467: Andrew Jackson 1834 Andrew Jackson 1835\n",
      "0.661191660427: William J. Clinton 1996 William J. Clinton 1997\n",
      "0.659815900499: George W. Bush 2006 George W. Bush 2008\n",
      "0.6586547376: William J. Clinton 1997 William J. Clinton 2000\n",
      "0.653409495117: Theodore Roosevelt 1907 Theodore Roosevelt 1908\n",
      "0.652785306081: Ronald Reagan 1984 Ronald Reagan 1985\n",
      "0.651306750972: Dwight D. Eisenhower 1954 Dwight D. Eisenhower 1956\n",
      "0.650223771649: George W. Bush 2005 George W. Bush 2006\n",
      "0.64802526417: Grover Cleveland 1885 Grover Cleveland 1886\n",
      "0.645891640459: James K. Polk 1845 James K. Polk 1846\n",
      "0.644816599114: George W. Bush 2003 George W. Bush 2008\n",
      "0.642262439163: Theodore Roosevelt 1905 Theodore Roosevelt 1907\n",
      "0.642244918833: Dwight D. Eisenhower 1954 Dwight D. Eisenhower 1955\n",
      "0.640882310662: Theodore Roosevelt 1906 Theodore Roosevelt 1907\n",
      "0.639347345545: George W. Bush 2004 George W. Bush 2006\n",
      "0.63738033663: Andrew Johnson 1867 Andrew Johnson 1868\n",
      "\n",
      "25 most similar pairs of presidents on average\n",
      "0.569172112357: William J. Clinton Barack Obama\n",
      "0.542148600813: George Bush Barack Obama\n",
      "0.524067996226: Ronald Reagan George Bush\n",
      "0.516784908621: George Bush William J. Clinton\n",
      "0.513242297279: Ronald Reagan Barack Obama\n",
      "0.496749445047: Zachary Taylor Millard Fillmore\n",
      "0.49260692094: Ronald Reagan William J. Clinton\n",
      "0.47680727677: Dwight D. Eisenhower John F. Kennedy\n",
      "0.436530302634: James K. Polk Millard Fillmore\n",
      "0.433923911463: Andrew Jackson Martin Van Buren\n",
      "0.433539748776: Gerald R. Ford Jimmy Carter\n",
      "0.433369492762: Rutherford B. Hayes Chester A. Arthur\n",
      "0.422196794903: Benjamin Harrison Grover Cleveland\n",
      "0.420301337735: Chester A. Arthur William Howard Taft\n",
      "0.418159689895: John F. Kennedy Gerald R. Ford\n",
      "0.417924887665: Jimmy Carter Ronald Reagan\n",
      "0.416810755075: Martin Van Buren John Tyler\n",
      "0.416353349542: Millard Fillmore Franklin Pierce\n",
      "0.415560370633: Rutherford B. Hayes Grover Cleveland\n",
      "0.415560370633: Rutherford B. Hayes Grover Cleveland\n",
      "0.414006179487: James K. Polk Zachary Taylor\n",
      "0.412607620675: George W. Bush Barack Obama\n",
      "0.409048682914: John F. Kennedy Jimmy Carter\n",
      "0.408833693674: Theodore Roosevelt William Howard Taft\n",
      "0.408157631295: Grover Cleveland Benjamin Harrison\n",
      "\n",
      "Exiting comparison code\n",
      "It appears that the SOUs tend to rate speeches which are temporally close by as more similar, reflecting what are likely similarities in the language and the issues being addressed at those times. For better similarity, it may be nessecary to not only consider the frequency of words, but also their relative spacing to other words. For example, the fact that a speech mentions America is important, but so is the grammatrical context in which it is mentioned in. Grammatical rules, however, are sophisticated, and as such, this is a difficult problem.\n",
      "Entering visualization /clustering code\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEVCAYAAAABwEUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsDczS4DD3swsAQ57M7MEOOzNzBLw/wEwx99m4575OQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbffa44ead0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import pylab\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext\n",
    "\n",
    "ORDERED_VOCAB_LIST = [] # READ_ONLY after initialization, to get a constant order on the weights vector\n",
    "MASTER_VOCAB_DICT = {} # READ_ONLY after initialization, maps word -> (num occurences, set(years))\n",
    "MASTER_DOC_TERMCOUNT_DICT = {} # READ_ONLY after initialization. # maps doc. year -> word -> occurence count\n",
    "\n",
    "MASTER_WEIGHTS_VECTORS_DICT = {} # READ_ONLY after initialization, maps doc. id -> ([vector], magnitude (of vector))\n",
    "\n",
    "MASTER_COMPARISON_SCORES_DICT = {} # READ_ONLY after initialization, maps doc. id 1 -> {doc. id 2 -> comparison of 1 and 2} \n",
    "\n",
    "NOT_IN_VOCAB_KEY = \"NOT_IN_VOCAB\"\n",
    "\n",
    "#### Data generation functions ####\n",
    "\n",
    "def wordCountFlatMap(row):\n",
    "    # replace non-alphanumeric with ' ', then split on runs of whitespace\n",
    "    originating_doc = str(row['year']).strip()\n",
    "    word_list = re.sub(r\"\\W+\", ' ', row['text']).lower().split()\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i] + originating_doc # encode document year in string \n",
    "    return word_list\n",
    "\n",
    "def wordCountMap(word):\n",
    "    originating_doc = word[-4:] # retrieve document year\n",
    "    doc_set = set()\n",
    "    doc_set.add(originating_doc)\n",
    "    return (word[:-4], (1, doc_set))\n",
    "\n",
    "def wordCountReduce(x, y): # to be called with reduce by key\n",
    "    return (x[0] + y[0], x[1] | y[1]);\n",
    "\n",
    "def initVocab(wordcount_list): # Should be called only ONCE\n",
    "    wordcount_list.sort(key = lambda x: x[1][0], reverse=True) # sort by wordcount\n",
    "    try:\n",
    "        common_cutoff = 20 - 1 # cutoff most common 20 words\n",
    "        uncommon_cutoff = 50 # cutoff all words with less than 50 appearances\n",
    "        for i in range(common_cutoff, len(wordcount_list)):\n",
    "            if wordcount_list[i][1][0] < uncommon_cutoff: # the list is sorted, so this means we have hit our cutoff\n",
    "                break\n",
    "            else:\n",
    "                MASTER_VOCAB_DICT[wordcount_list[i][0]] = wordcount_list[i][1] # maps word -> (num occurences, set(years))\n",
    "                ORDERED_VOCAB_LIST.append(wordcount_list[i][0])\n",
    "        return\n",
    "                \n",
    "    except IndexError:\n",
    "        print \"[ERROR]: Data malformed or cutoff bound incorrect.\"\n",
    "        \n",
    "def docOccurenceMap(word): # operates on output from wordCountFlatMap\n",
    "    originating_doc = word[-4:] # retrieve document year\n",
    "    occ_dict = {}\n",
    "    occ_dict[word[:-4]] = 1\n",
    "    return (originating_doc, occ_dict)\n",
    "\n",
    "# to be called with reduceByKey, creates dict of form word -> count for a given doc.\n",
    "# DOES NOT filter on vocabulary\n",
    "# Fairly efficient, new keys are written only when needed, old values are summed\n",
    "def docOccurenceReduce(x, y):\n",
    "    if len(x) > len(y): # we do not want to copy values into the smaller dict from the larger\n",
    "        for key in y:\n",
    "            if key in x:\n",
    "                x[key] += y[key]\n",
    "            else:\n",
    "                x[key] = 1\n",
    "        return x\n",
    "    else:\n",
    "        for key in x:\n",
    "            if key in y:\n",
    "                y[key] += x[key]\n",
    "            else:\n",
    "                y[key] = 1\n",
    "        return y\n",
    "    \n",
    "def initDocTermCounts(doc_wordcount_list):\n",
    "    for key, wordcount_dict in doc_wordcount_list:\n",
    "        # this is another dictionary of the form word -> count\n",
    "        MASTER_DOC_TERMCOUNT_DICT[key] = wordcount_dict\n",
    "        \n",
    "    return\n",
    "\n",
    "def doVectorWeightsMap(word): # operates on output from wordCountFlatMap, returns (doc. year, word -> weight (float)))\n",
    "    originating_doc = word[-4:] # retrieve document year\n",
    "    decoded_word = word[:-4]\n",
    "    return_dict = {}\n",
    "    if decoded_word in MASTER_VOCAB_DICT: # is the word in the vocabulary\n",
    "        numOccurencesInDoc = 0\n",
    "        if decoded_word in MASTER_DOC_TERMCOUNT_DICT[originating_doc]: # is word in document\n",
    "            numOccurencesInDoc = MASTER_DOC_TERMCOUNT_DICT[originating_doc][decoded_word]\n",
    "        else: # save ourselves some computation, return 0\n",
    "            return_dict[decoded_word] = 0.0\n",
    "            return (originating_doc, return_dict) \n",
    "        \n",
    "        collectionDocTotal = len(MASTER_DOC_TERMCOUNT_DICT)\n",
    "        numDocsContainingWord = len(MASTER_VOCAB_DICT[decoded_word][1]) # length of the set of all documents which have a term t\n",
    "        \n",
    "        weight = 0.0\n",
    "        if numDocsContainingWord > 0: # safety check, should never fail\n",
    "            weight = numOccurencesInDoc * math.log(float(collectionDocTotal) / float(numDocsContainingWord))\n",
    "        else:\n",
    "            print \"ERROR: no documents contain word weight is being calculated for\"\n",
    "        \n",
    "        return_dict[decoded_word] = weight    \n",
    "        return (originating_doc, return_dict)\n",
    "        \n",
    "    else:\n",
    "        return_dict[decoded_word] = -1.0\n",
    "        return (NOT_IN_VOCAB_KEY, return_dict)\n",
    "    \n",
    "def doVectorWeightsReduce(x, y): # to be called with reduceByKey\n",
    "    if len(x) > len(y): # shorter dictionary is used to minimize iterations\n",
    "        for key in y:\n",
    "            if key not in x:\n",
    "                x[key] = y[key]\n",
    "\n",
    "        return x\n",
    "    else:\n",
    "        for key in x:\n",
    "            if key not in y:\n",
    "                y[key] = x[key]\n",
    "\n",
    "        return y\n",
    "\n",
    "def initWeightVectors(unordered_weights_list):\n",
    "    for doc_id, weights_dict in unordered_weights_list:\n",
    "        if doc_id != NOT_IN_VOCAB_KEY:\n",
    "            vector = []\n",
    "            for word in ORDERED_VOCAB_LIST:\n",
    "                try: # order the vectors in some consistent but arbitrary ordering of the vocabulary\n",
    "                    vector.append(weights_dict[word])\n",
    "                except KeyError: # attempted to look a word which is in the vocabulary, but not this document\n",
    "                    vector.append(0.0)\n",
    "\n",
    "            magnitude = np.linalg.norm(vector)\n",
    "            MASTER_WEIGHTS_VECTORS_DICT[doc_id] = (vector, magnitude)\n",
    "\n",
    "    return\n",
    "\n",
    "#### Data Comparision/Visualization Functions ####\n",
    "\n",
    "def calculateComparisonsMap(row): # returns a k,v pair of (doc. id 1 -> {doc. id != 1 -> score})\n",
    "    comparison_dict = {}\n",
    "    outer_sou_key = row['year'] # only reason we are mapping using the original rdd\n",
    "\n",
    "    # TODO: create ordered keylist of MASTER_WEIGHTS_VECTORS_DICT? Returning dict so not nessecary?\n",
    "    for inner_sou_key in MASTER_WEIGHTS_VECTORS_DICT:\n",
    "        if outer_sou_key != inner_sou_key:\n",
    "            numerator = np.dot(MASTER_WEIGHTS_VECTORS_DICT[outer_sou_key][0],\n",
    "                               MASTER_WEIGHTS_VECTORS_DICT[inner_sou_key][0])\n",
    "            # precomputed magnitudes\n",
    "            denominator = MASTER_WEIGHTS_VECTORS_DICT[outer_sou_key][1] * \\\n",
    "                          MASTER_WEIGHTS_VECTORS_DICT[inner_sou_key][1]\n",
    "\n",
    "            comparison_dict[inner_sou_key] = float(numerator) / float(denominator)\n",
    "                                   \n",
    "    return (outer_sou_key, comparison_dict)\n",
    "\n",
    "\n",
    "    \n",
    "######################################################################################################################\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "word_rdd = df.rdd.flatMap(wordCountFlatMap)\n",
    "wordcount_list = word_rdd.map(wordCountMap).reduceByKey(wordCountReduce).collect()\n",
    "initVocab(wordcount_list) # MASTER_VOCAB_DICT is read_only at this point\n",
    "doc_wordcount_list = word_rdd.map(docOccurenceMap).reduceByKey(docOccurenceReduce).collect() # term counts per doc\n",
    "initDocTermCounts(doc_wordcount_list)\n",
    "\n",
    "unordered_weights_list = word_rdd.map(doVectorWeightsMap).reduceByKey(doVectorWeightsReduce).collect()\n",
    "\n",
    "initWeightVectors(unordered_weights_list)\n",
    "\n",
    "########### Comparison code ############\n",
    "print \"Entering comparison code\"\n",
    "\n",
    "# make sure the order of keys is consistent for the double loop. Must be ordered on years\n",
    "years_list = []\n",
    "# make sure the order of keys is consistent for the double loop. Must be ordered on year for code to work\n",
    "pres_list = []\n",
    "\n",
    "presidential_mapping_dict = {}\n",
    "for row in df.rdd.collect(): # initialize some helpful data structs\n",
    "    years_list.append(row['year'])  \n",
    "    presidential_mapping_dict[row['year']] = row['president']\n",
    "\n",
    "years_list.sort()\n",
    "for year in years_list:\n",
    "    president = presidential_mapping_dict[year]\n",
    "    if len(pres_list) == 0:\n",
    "        pres_list.append(president)\n",
    "    else:\n",
    "        if pres_list[-1] != president:\n",
    "            pres_list.append(president)\n",
    "\n",
    "# print MASTER_WEIGHTS_VECTORS_DICT\n",
    "\n",
    "comparisons_list = df.rdd.map(calculateComparisonsMap).collect()\n",
    "for outer_sou_key, comparison_dict in comparisons_list:\n",
    "    MASTER_COMPARISON_SCORES_DICT[outer_sou_key] = comparison_dict\n",
    "        \n",
    "    \n",
    "# compare\n",
    "diff_president_sou = []\n",
    "same_president_sou = []\n",
    "\n",
    "averaged_similarity_dict = {}\n",
    "# NOTE: using indexes so we do not get duplicates in the intermediate results (i.e compare 1900 with 2000, 2000 with 1900)\n",
    "for i in range(len(years_list)):\n",
    "    for j in range(i + 1, len(years_list)):\n",
    "        outer_year = years_list[i]\n",
    "        inner_year = years_list[j]\n",
    "        if (outer_year != inner_year):\n",
    "            outer_president = presidential_mapping_dict[outer_year]\n",
    "            inner_president = presidential_mapping_dict[inner_year]\n",
    "            if (outer_president == inner_president): # are presidents the same\n",
    "                same_president_sou.append((MASTER_COMPARISON_SCORES_DICT[outer_year][inner_year],\n",
    "                                          (outer_president, outer_year, inner_president, inner_year))) # get comparison scores\n",
    "            else:\n",
    "                diff_president_sou.append((MASTER_COMPARISON_SCORES_DICT[outer_year][inner_year],\n",
    "                                          (outer_president, outer_year, inner_president, inner_year))) # get comparison scores\n",
    "                                          \n",
    "                # handle averaging scores between presidents\n",
    "                if outer_president not in averaged_similarity_dict:\n",
    "                    averaged_similarity_dict[outer_president] = {}\n",
    "                if inner_president not in averaged_similarity_dict[outer_president]:\n",
    "                    averaged_similarity_dict[outer_president][inner_president] = [0.0, 0]\n",
    "                averaged_similarity_dict[outer_president][inner_president][0] += \\\n",
    "                                          MASTER_COMPARISON_SCORES_DICT[outer_year][inner_year] # update score count\n",
    "                averaged_similarity_dict[outer_president][inner_president][1] += 1 # update count\n",
    "                                          \n",
    "\n",
    "diff_president_sou.sort(key = lambda x: x[0], reverse=True) # sort by score, highest first\n",
    "same_president_sou.sort(key = lambda x: x[0], reverse=True) # sort by score, highest first\n",
    "\n",
    "avg_presidents_sou = []\n",
    "for i in range(len(pres_list)):\n",
    "    for j in range(i + 1, len(pres_list)):\n",
    "        if (pres_list[i] != pres_list[j]): # handles Grover Cleveland and his non-consecutive terms\n",
    "            numerator = averaged_similarity_dict[pres_list[i]][pres_list[j]][0]\n",
    "            denominator = averaged_similarity_dict[pres_list[i]][pres_list[j]][1]\n",
    "            avg_presidents_sou.append((float(numerator) / float(denominator), pres_list[i], pres_list[j]))\n",
    "            \n",
    "avg_presidents_sou.sort(key = lambda x: x[0], reverse=True) # sort by score, highest first\n",
    "\n",
    "### RESULTS ###\n",
    "print \"50 most similar pairs of speeches by different presidents\"\n",
    "for score, names_tuple in diff_president_sou[:50]:\n",
    "    print str(score) + \": \" + \" \".join(str(x) for x in names_tuple)\n",
    "print\n",
    "print \"50 most similar pairs of speeches by the same presidents\"\n",
    "for score, names_tuple in same_president_sou[:50]:\n",
    "    print str(score) + \": \" + \" \".join(str(x) for x in names_tuple)\n",
    "print\n",
    "print \"25 most similar pairs of presidents on average\"\n",
    "for data_tuple in avg_presidents_sou[:25]:\n",
    "    print str(data_tuple[0]) + \": \" + data_tuple[1] + \" \" + data_tuple[2]\n",
    "print\n",
    "\n",
    "            \n",
    "print \"Exiting comparison code\"\n",
    "# CHECK: Make sure that the below statement is correct\n",
    "print \"It appears that the SOUs tend to rate speeches which are temporally close by as more similar, reflecting what are likely \\\n",
    "similarities in the language and the issues being addressed at those times. \\\n",
    "For better similarity, it may be nessecary to not only consider the frequency of words, but also their relative spacing \\\n",
    "to other words. For example, the fact that a speech mentions America is important, but so is the grammatrical context in which \\\n",
    "it is mentioned in. Grammatical rules, however, are sophisticated, and as such, this is a difficult problem.\"\n",
    "############################################################\n",
    "\n",
    "print \"Entering visualization /clustering code\"\n",
    "\n",
    "pre_rdd_list = []\n",
    "for doc_id in years_list:\n",
    "    # label = doc_id\n",
    "    data = MASTER_WEIGHTS_VECTORS_DICT[doc_id][0] # pull out tf-idf weights\n",
    "    # pre_rdd_list.append(LabeledPoint(label, data))\n",
    "    pre_rdd_list.append(data)\n",
    "    \n",
    "\n",
    "train_data = spark.sparkContext.parallelize(pre_rdd_list, len(pre_rdd_list))\n",
    "\n",
    "clusters = KMeans.train(train_data, 10, maxIterations=50, runs=10, initializationMode=\"random\")\n",
    " \n",
    "cluster_list = clusters.predict(train_data).collect() # assigned clusters for speech, ordered by speech year \n",
    "\n",
    "plt.clf()\n",
    "plt.figure(1)\n",
    " \n",
    "plt.scatter(years_list, cluster_list, s = 10)\n",
    "plt.title('SOU Year vs Predicted Cluster', fontsize = 12)\n",
    "plt.xlabel('Year', fontsize = 8)\n",
    "plt.ylabel('Cluster', fontsize = 8)\n",
    " \n",
    "plt.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
