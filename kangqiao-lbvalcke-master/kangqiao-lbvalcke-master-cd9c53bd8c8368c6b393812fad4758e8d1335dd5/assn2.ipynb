{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering comparison code\n",
      "[(0.5691721123565571, u'William J. Clinton', u'Barack Obama'), (0.5421486008128538, u'George Bush', u'Barack Obama'), (0.5240679962256453, u'Ronald Reagan', u'George Bush'), (0.5167849086206793, u'George Bush', u'William J. Clinton'), (0.5132422972792439, u'Ronald Reagan', u'Barack Obama'), (0.49674944504733715, u'Zachary Taylor', u'Millard Fillmore'), (0.4926069209398873, u'Ronald Reagan', u'William J. Clinton'), (0.47680727677036994, u'Dwight D. Eisenhower', u'John F. Kennedy'), (0.436530302633883, u'James K. Polk', u'Millard Fillmore'), (0.43392391146349973, u'Andrew Jackson', u'Martin Van Buren'), (0.43353974877563584, u'Gerald R. Ford', u'Jimmy Carter'), (0.43336949276245934, u'Rutherford B. Hayes', u'Chester A. Arthur'), (0.4221967949030207, u'Benjamin Harrison', u'Grover Cleveland'), (0.420301337734583, u'Chester A. Arthur', u'William Howard Taft'), (0.41815968989517316, u'John F. Kennedy', u'Gerald R. Ford'), (0.41792488766471414, u'Jimmy Carter', u'Ronald Reagan'), (0.41681075507541193, u'Martin Van Buren', u'John Tyler'), (0.4163533495424043, u'Millard Fillmore', u'Franklin Pierce'), (0.41556037063272694, u'Rutherford B. Hayes', u'Grover Cleveland'), (0.41556037063272694, u'Rutherford B. Hayes', u'Grover Cleveland'), (0.4140061794866226, u'James K. Polk', u'Zachary Taylor'), (0.4126076206746584, u'George W. Bush', u'Barack Obama'), (0.4090486829144712, u'John F. Kennedy', u'Jimmy Carter'), (0.40883369367440486, u'Theodore Roosevelt', u'William Howard Taft'), (0.40815763129530813, u'Grover Cleveland', u'Benjamin Harrison')]\n",
      "Exiting comparison code\n",
      "It appears that the SOUs tend to rate speeches which are temporally close by as more similar, reflecting what are likely similarities in the language and the issues being addressed at those times. For better similarity, it may be nessecary to not only consider the frequency of words, but also their relative spacing to other words. For example, the fact that a speech mentions America is important, but so is the grammatrical context in which it is mentioned in. Grammatical rules, however, are sophisticated, and as such, this is a difficult problem.\n",
      "Entering visualization /clustering code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spark-2.1-el7-x86_64/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEVCAYAAAABwEUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c8717e9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import pylab\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext\n",
    "\n",
    "ORDERED_VOCAB_LIST = [] # READ_ONLY after initialization, to get a constant order on the weights vector\n",
    "MASTER_VOCAB_DICT = {} # READ_ONLY after initialization, maps word -> (num occurences, set(years))\n",
    "MASTER_DOC_TERMCOUNT_DICT = {} # READ_ONLY after initialization. # maps doc. year -> word -> occurence count\n",
    "\n",
    "MASTER_WEIGHTS_VECTORS_DICT = {} # READ_ONLY after initialization, maps doc. id -> ([vector], magnitude (of vector))\n",
    "\n",
    "MASTER_COMPARISON_SCORES_DICT = {} # READ_ONLY after initialization, maps doc. id 1 -> {doc. id 2 -> comparison of 1 and 2} \n",
    "\n",
    "NOT_IN_VOCAB_KEY = \"NOT_IN_VOCAB\"\n",
    "\n",
    "#### Data generation functions ####\n",
    "\n",
    "def wordCountFlatMap(row):\n",
    "    # replace non-alphanumeric with ' ', then split on runs of whitespace\n",
    "    originating_doc = str(row['year']).strip()\n",
    "    word_list = re.sub(r\"\\W+\", ' ', row['text']).lower().split()\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i] + originating_doc # encode document year in string \n",
    "    # s = s.lower().translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    return word_list\n",
    "\n",
    "def wordCountMap(word):\n",
    "    originating_doc = word[-4:] # retrieve document year\n",
    "    doc_set = set()\n",
    "    doc_set.add(originating_doc)\n",
    "    return (word[:-4], (1, doc_set))\n",
    "\n",
    "def wordCountReduce(x, y): # to be called with reduce by key\n",
    "    return (x[0] + y[0], x[1] | y[1]);\n",
    "\n",
    "def initVocab(wordcount_list): # Should be called only ONCE\n",
    "    wordcount_list.sort(key = lambda x: x[1][0], reverse=True) # sort by wordcount\n",
    "    try:\n",
    "        common_cutoff = 20 - 1 # cutoff most common 20 words\n",
    "        uncommon_cutoff = 50 # cutoff all words with less than 50 appearances\n",
    "        for i in range(common_cutoff, len(wordcount_list)):\n",
    "            if wordcount_list[i][1][0] < uncommon_cutoff: # the list is sorted, so this means we have hit our cutoff\n",
    "                break\n",
    "            else:\n",
    "                MASTER_VOCAB_DICT[wordcount_list[i][0]] = wordcount_list[i][1] # maps word -> (num occurences, set(years))\n",
    "                ORDERED_VOCAB_LIST.append(wordcount_list[i][0])\n",
    "        return\n",
    "                \n",
    "    except IndexError:\n",
    "        print \"[ERROR]: Data malformed or cutoff bound incorrect.\"\n",
    "        \n",
    "# def initVocabOrder():\n",
    "#     ORDERED_VOCAB_LIST = MASTER_VOCAB_DICT.keys()\n",
    "#     return\n",
    "        \n",
    "def docOccurenceMap(word): # operates on output from wordCountFlatMap\n",
    "    originating_doc = word[-4:] # retrieve document year\n",
    "    occ_dict = {}\n",
    "    occ_dict[word[:-4]] = 1\n",
    "    return (originating_doc, occ_dict)\n",
    "\n",
    "# to be called with reduceByKey, creates dict of form word -> count for a given doc.\n",
    "# DOES NOT filter on vocabulary\n",
    "# Fairly efficient, new keys are written only when needed, old values are summed\n",
    "def docOccurenceReduce(x, y):\n",
    "    if len(x) > len(y): # we do not want to copy values into the smaller dict from the larger\n",
    "        for key in y:\n",
    "            if key in x:\n",
    "                x[key] += y[key]\n",
    "            else:\n",
    "                x[key] = 1\n",
    "        return x\n",
    "    else:\n",
    "        for key in x:\n",
    "            if key in y:\n",
    "                y[key] += x[key]\n",
    "            else:\n",
    "                y[key] = 1\n",
    "        return y\n",
    "    \n",
    "def initDocTermCounts(doc_wordcount_list):\n",
    "    for key, wordcount_dict in doc_wordcount_list:\n",
    "        # this is another dictionary of the form word -> count\n",
    "        MASTER_DOC_TERMCOUNT_DICT[key] = wordcount_dict\n",
    "        \n",
    "    return\n",
    "\n",
    "def doVectorWeightsMap(word): # operates on output from wordCountFlatMap, returns (doc. year, word -> weight (float)))\n",
    "    originating_doc = word[-4:] # retrieve document year\n",
    "    decoded_word = word[:-4]\n",
    "    return_dict = {}\n",
    "    if decoded_word in MASTER_VOCAB_DICT: # is the word in the vocabulary\n",
    "        numOccurencesInDoc = 0\n",
    "        if decoded_word in MASTER_DOC_TERMCOUNT_DICT[originating_doc]: # is word in document\n",
    "            numOccurencesInDoc = MASTER_DOC_TERMCOUNT_DICT[originating_doc][decoded_word]\n",
    "        else: # save ourselves some computation, return 0\n",
    "            return_dict[decoded_word] = 0.0\n",
    "            return (originating_doc, return_dict) \n",
    "        \n",
    "        collectionDocTotal = len(MASTER_DOC_TERMCOUNT_DICT)\n",
    "        numDocsContainingWord = len(MASTER_VOCAB_DICT[decoded_word][1]) # length of the set of all documents which have a term t\n",
    "        \n",
    "        weight = 0.0\n",
    "        if numDocsContainingWord > 0: # safety check, should never fail\n",
    "            weight = numOccurencesInDoc * math.log(float(collectionDocTotal) / float(numDocsContainingWord))\n",
    "        else:\n",
    "            print \"ERROR: no documents contain word weight is being calculated for\"\n",
    "        \n",
    "        return_dict[decoded_word] = weight    \n",
    "        return (originating_doc, return_dict)\n",
    "        \n",
    "    else:\n",
    "        return_dict[decoded_word] = -1.0\n",
    "        return (NOT_IN_VOCAB_KEY, return_dict)\n",
    "    \n",
    "def doVectorWeightsReduce(x, y): # to be called with reduceByKey\n",
    "    if len(x) > len(y): # shorter dictionary is used to minimize iterations\n",
    "        for key in y:\n",
    "            if key not in x:\n",
    "                x[key] = y[key]\n",
    "\n",
    "        return x\n",
    "    else:\n",
    "        for key in x:\n",
    "            if key not in y:\n",
    "                y[key] = x[key]\n",
    "\n",
    "        return y\n",
    "\n",
    "def initWeightVectors(unordered_weights_list):\n",
    "    for doc_id, weights_dict in unordered_weights_list:\n",
    "        if doc_id != NOT_IN_VOCAB_KEY:\n",
    "            vector = []\n",
    "            for word in ORDERED_VOCAB_LIST:\n",
    "                try: # order the vectors in some consistent but arbitrary ordering of the vocabulary\n",
    "                    vector.append(weights_dict[word])\n",
    "                except KeyError: # attempted to look a word which is in the vocabulary, but not this document\n",
    "                    # print \"[NOTE] attempted to find word \" + word + \" in weights_dict for year \" + \\\n",
    "                    # doc_id + \" but failed\"\n",
    "                    vector.append(0.0)\n",
    "\n",
    "            magnitude = np.linalg.norm(vector)\n",
    "            MASTER_WEIGHTS_VECTORS_DICT[doc_id] = (vector, magnitude)\n",
    "\n",
    "    return\n",
    "\n",
    "#### Data Comparision/Visualization Functions ####\n",
    "\n",
    "def calculateComparisonsMap(row): # returns a k,v pair of (doc. id 1 -> {doc. id != 1 -> score})\n",
    "    comparison_dict = {}\n",
    "    outer_sou_key = row['year'] # only reason we are mapping using the original rdd\n",
    "\n",
    "    # TODO: create ordered keylist of MASTER_WEIGHTS_VECTORS_DICT? Returning dict so not nessecary?\n",
    "    for inner_sou_key in MASTER_WEIGHTS_VECTORS_DICT:\n",
    "        if outer_sou_key != inner_sou_key:\n",
    "            numerator = np.dot(MASTER_WEIGHTS_VECTORS_DICT[outer_sou_key][0],\n",
    "                               MASTER_WEIGHTS_VECTORS_DICT[inner_sou_key][0])\n",
    "            # precomputed magnitudes\n",
    "            denominator = MASTER_WEIGHTS_VECTORS_DICT[outer_sou_key][1] * \\\n",
    "                          MASTER_WEIGHTS_VECTORS_DICT[inner_sou_key][1]\n",
    "\n",
    "            comparison_dict[inner_sou_key] = float(numerator) / float(denominator)\n",
    "                                   \n",
    "    return (outer_sou_key, comparison_dict)\n",
    "\n",
    "\n",
    "    \n",
    "######################################################################################################################\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "#print df.rdd.flatMap(wordCountFlatMap).map(wordCountMap).collect()\n",
    "word_rdd = df.rdd.flatMap(wordCountFlatMap)\n",
    "wordcount_list = word_rdd.map(wordCountMap).reduceByKey(wordCountReduce).collect()\n",
    "initVocab(wordcount_list) # MASTER_VOCAB_DICT is read_only at this point\n",
    "# initVocabOrder() # ORDERED_VOCAB_LIST is now read_only\n",
    "# print MASTER_VOCAB_DICT\n",
    "doc_wordcount_list = word_rdd.map(docOccurenceMap).reduceByKey(docOccurenceReduce).collect() # term counts per doc\n",
    "initDocTermCounts(doc_wordcount_list)\n",
    "\n",
    "unordered_weights_list = word_rdd.map(doVectorWeightsMap).reduceByKey(doVectorWeightsReduce).collect()\n",
    "\n",
    "initWeightVectors(unordered_weights_list)\n",
    "# print MASTER_WEIGHTS_VECTORS_DICT\n",
    "# print 'america' in MASTER_VOCAB_DICT\n",
    "# print '1830' in MASTER_VOCAB_DICT['america'][1]\n",
    "\n",
    "########### Comparison code ############\n",
    "print \"Entering comparison code\"\n",
    "\n",
    "# make sure the order of keys is consistent for the double loop. Must be ordered on years\n",
    "years_list = []\n",
    "# make sure the order of keys is consistent for the double loop. Must be ordered on year for code to work\n",
    "pres_list = []\n",
    "\n",
    "presidential_mapping_dict = {}\n",
    "for row in df.rdd.collect(): # initialize some helpful data structs\n",
    "    years_list.append(row['year'])  \n",
    "    presidential_mapping_dict[row['year']] = row['president']\n",
    "\n",
    "years_list.sort()\n",
    "for year in years_list:\n",
    "    president = presidential_mapping_dict[year]\n",
    "    if len(pres_list) == 0:\n",
    "        pres_list.append(president)\n",
    "    else:\n",
    "        if pres_list[-1] != president:\n",
    "#             if president == u'Grover Cleveland' and grover_count == 0: # only president to serve non-continous terms\n",
    "#                 print \"ENTERING GROVER CHECK\"\n",
    "#                 pres_list.append(president)\n",
    "#                 grover_count = 1\n",
    "#             else:\n",
    "            pres_list.append(president)\n",
    "\n",
    "# print MASTER_WEIGHTS_VECTORS_DICT\n",
    "\n",
    "comparisons_list = df.rdd.map(calculateComparisonsMap).collect()\n",
    "for outer_sou_key, comparison_dict in comparisons_list:\n",
    "    MASTER_COMPARISON_SCORES_DICT[outer_sou_key] = comparison_dict\n",
    "#     president = presidential_mapping_dict[outer_sou_key]\n",
    "#     if president not in MASTER_COMPARISON_SCORES_DICT:\n",
    "#         MASTER_COMPARISON_SCORES_DICT[president] = {}\n",
    "#         MASTER_COMPARISON_SCORES_DICT[president][outer_sou_key] = comparison_dict\n",
    "#         comparison_dict\n",
    "#     else:\n",
    "        \n",
    "    \n",
    "# compare\n",
    "diff_president_sou = []\n",
    "same_president_sou = []\n",
    "\n",
    "averaged_similarity_dict = {}\n",
    "# NOTE: using indexes so we do not get duplicates in the intermediate results (i.e compare 1900 with 2000, 2000 with 1900)\n",
    "for i in range(len(years_list)):\n",
    "    for j in range(i + 1, len(years_list)):\n",
    "        outer_year = years_list[i]\n",
    "        inner_year = years_list[j]\n",
    "        if (outer_year != inner_year):\n",
    "            outer_president = presidential_mapping_dict[outer_year]\n",
    "            inner_president = presidential_mapping_dict[inner_year]\n",
    "            if (outer_president == inner_president): # are presidents the same\n",
    "                same_president_sou.append((MASTER_COMPARISON_SCORES_DICT[outer_year][inner_year],\n",
    "                                          (outer_president, outer_year, inner_president, inner_year))) # get comparison scores\n",
    "            else:\n",
    "                diff_president_sou.append((MASTER_COMPARISON_SCORES_DICT[outer_year][inner_year],\n",
    "                                          (outer_president, outer_year, inner_president, inner_year))) # get comparison scores\n",
    "                                          \n",
    "                # handle averaging scores between presidents\n",
    "                if outer_president not in averaged_similarity_dict:\n",
    "                    averaged_similarity_dict[outer_president] = {}\n",
    "                if inner_president not in averaged_similarity_dict[outer_president]:\n",
    "                    averaged_similarity_dict[outer_president][inner_president] = [0.0, 0]\n",
    "                averaged_similarity_dict[outer_president][inner_president][0] += \\\n",
    "                                          MASTER_COMPARISON_SCORES_DICT[outer_year][inner_year] # update score count\n",
    "                averaged_similarity_dict[outer_president][inner_president][1] += 1 # update count\n",
    "                                          \n",
    "\n",
    "diff_president_sou.sort(key = lambda x: x[0], reverse=True) # sort by score, highest first\n",
    "same_president_sou.sort(key = lambda x: x[0], reverse=True) # sort by score, highest first\n",
    "\n",
    "\n",
    "# print averaged_similarity_dict\n",
    "# print years_list\n",
    "# print pres_list\n",
    "\n",
    "avg_presidents_sou = []\n",
    "for i in range(len(pres_list)):\n",
    "    for j in range(i + 1, len(pres_list)):\n",
    "        if (pres_list[i] != pres_list[j]): # handles Grover Cleveland and his non-consecutive terms\n",
    "            numerator = averaged_similarity_dict[pres_list[i]][pres_list[j]][0]\n",
    "            denominator = averaged_similarity_dict[pres_list[i]][pres_list[j]][1]\n",
    "            avg_presidents_sou.append((float(numerator) / float(denominator), pres_list[i], pres_list[j]))\n",
    "            \n",
    "avg_presidents_sou.sort(key = lambda x: x[0], reverse=True) # sort by score, highest first\n",
    "\n",
    "### RESULTS ###\n",
    "# print diff_president_sou[:50]\n",
    "# print same_president_sou[:50]\n",
    "print avg_presidents_sou[:25]\n",
    "\n",
    "\n",
    "            \n",
    "print \"Exiting comparison code\"\n",
    "# CHECK: Make sure that the below statement is correct\n",
    "print \"It appears that the SOUs tend to rate speeches which are temporally close by as more similar, reflecting what are likely \\\n",
    "similarities in the language and the issues being addressed at those times. \\\n",
    "For better similarity, it may be nessecary to not only consider the frequency of words, but also their relative spacing \\\n",
    "to other words. For example, the fact that a speech mentions America is important, but so is the grammatrical context in which \\\n",
    "it is mentioned in. Grammatical rules, however, are sophisticated, and as such, this is a difficult problem.\"\n",
    "############################################################\n",
    "\n",
    "print \"Entering visualization /clustering code\"\n",
    "\n",
    "pre_rdd_list = []\n",
    "for doc_id in years_list:\n",
    "    # label = doc_id\n",
    "    data = MASTER_WEIGHTS_VECTORS_DICT[doc_id][0] # pull out tf-idf weights\n",
    "    # pre_rdd_list.append(LabeledPoint(label, data))\n",
    "    pre_rdd_list.append(data)\n",
    "    \n",
    "\n",
    "train_data = spark.sparkContext.parallelize(pre_rdd_list, len(pre_rdd_list))\n",
    "\n",
    "clusters = KMeans.train(train_data, 10, maxIterations=50, runs=10, initializationMode=\"random\")\n",
    " \n",
    "cluster_list = clusters.predict(train_data).collect() # assigned clusters for speech, ordered by speech year \n",
    "\n",
    "plt.clf()\n",
    "plt.figure(1)\n",
    " \n",
    "plt.scatter(years_list, cluster_list, s = 10)\n",
    "plt.title('SOU Year vs Predicted Cluster', fontsize = 12)\n",
    "plt.xlabel('Year', fontsize = 8)\n",
    "plt.ylabel('Cluster', fontsize = 8)\n",
    " \n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2830\n",
      "t of europe.\r\n",
      "\r\n",
      "the claims of american citizens fo\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def wordCountFlatMap(row):\n",
    "    # replace non-alphanumeric with ' ', then split on runs of whitespace\n",
    "    originating_doc = str(row['year']).strip()\n",
    "    word_list = re.sub(r\"\\W+\", ' ', row['text']).lower().split()\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i] + originating_doc # encode document year in string \n",
    "    # s = s.lower().translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    return word_list\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "test_string_4 = \"\"\n",
    "word_list = []\n",
    "for row in df.collect():\n",
    "    if row['year'] == '1836':\n",
    "        test_string_4 = row['text']\n",
    "        word_list = wordCountFlatMap(row)\n",
    "        \n",
    "tmp1 = test_string_4.lower()\n",
    "print tmp1.find('america')\n",
    "print tmp1[2800:2850]\n",
    "print 'america' in word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Master web UI: http://10.50.221.198:8080\n",
      "Entering part 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spark-2.1-el7-x86_64/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster num. 0 has primary label 7 : Samples of actual labels from cluster\n",
      "[7, 9, 4, 9, 4, 9, 7, 7, 7, 7, 7, 4, 9, 7, 9, 9, 8, 9, 4, 7, 4, 4, 7, 8, 9, 9, 7, 7, 9, 7]\n",
      "Percentage majority label: 12.0\n",
      "Cluster num. 1 has primary label 3 : Samples of actual labels from cluster\n",
      "[5, 3, 3, 3, 6, 3, 3, 3, 3, 8, 0, 0, 3, 3, 5, 3, 3, 3, 2, 5, 3, 0, 8, 8, 3, 5, 0, 3, 5, 3]\n",
      "Percentage majority label: 16.0\n",
      "Cluster num. 2 has primary label 6 : Samples of actual labels from cluster\n",
      "[6, 6, 6, 6, 6, 6, 6, 2, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 0]\n",
      "Percentage majority label: 26.0\n",
      "Cluster num. 3 has primary label 4 : Samples of actual labels from cluster\n",
      "[2, 4, 4, 2, 7, 7, 3, 4, 4, 8, 4, 4, 5, 4, 9, 4, 8, 4, 1, 7, 4, 7, 7, 4, 4, 6, 9, 2, 9, 9]\n",
      "Percentage majority label: 12.0\n",
      "Cluster num. 4 has primary label 8 : Samples of actual labels from cluster\n",
      "[8, 8, 8, 5, 5, 8, 7, 8, 8, 5, 3, 3, 8, 8, 8, 5, 8, 8, 8, 8, 8, 3, 2, 8, 8, 8, 5, 3, 5, 8]\n",
      "Percentage majority label: 18.0\n",
      "Cluster num. 5 has primary label 7 : Samples of actual labels from cluster\n",
      "[9, 9, 4, 4, 4, 4, 4, 7, 9, 7, 9, 7, 9, 7, 7, 9, 7, 9, 7, 9, 7, 4, 9, 9, 4, 7, 9, 8, 9, 4]\n",
      "Percentage majority label: 9.0\n",
      "Cluster num. 6 has primary label 2 : Samples of actual labels from cluster\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2]\n",
      "Percentage majority label: 26.0\n",
      "Cluster num. 7 has primary label 1 : Samples of actual labels from cluster\n",
      "[3, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 3, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 5]\n",
      "Percentage majority label: 23.0\n",
      "Cluster num. 8 has primary label 0 : Samples of actual labels from cluster\n",
      "[0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Percentage majority label: 28.0\n",
      "Cluster num. 9 has primary label 6 : Samples of actual labels from cluster\n",
      "[0, 3, 6, 0, 6, 0, 6, 3, 5, 6, 6, 0, 5, 8, 6, 5, 6, 0, 0, 6, 6, 6, 6, 6, 3, 3, 5, 5, 6, 6]\n",
      "Percentage majority label: 14.0\n",
      "Entering part 3\n"
     ]
    }
   ],
   "source": [
    "### Part 3 ###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import pylab\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, IndexedRow, IndexedRowMatrix\n",
    "from pyspark.mllib.feature import PCA\n",
    "\n",
    "# Globals for visualize()\n",
    "N_ROWS = 4\n",
    "N_COLS = 5\n",
    "\n",
    "NUM_PARTITIONS = 40\n",
    "\n",
    "# Part 1 functions\n",
    "\n",
    "# input: subset, an np array where the rows are digits\n",
    "# input: title, plot title\n",
    "# inputL savepath, filepath to save plot, Default None if user does not wish to save plot\n",
    "def visualize(subset, title, savepath=None):\n",
    "    nrows = 4\n",
    "    ncols = 5\n",
    "\n",
    "    plt.clf()\n",
    "    pylab.figure() # clear plot\n",
    "    plt.figure(figsize=(N_COLS*2, N_ROWS*2))\n",
    "\n",
    "    #for i in xrange(nrows*ncols):\n",
    "    for i in range(len(subset)):\n",
    "        plt.subplot(nrows, ncols, i+1)\n",
    "        plt.imshow(subset[i].reshape((28,28)), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "# Returns every item in rdd with column label affixed in a flat list\n",
    "def columnFlatMap(row):\n",
    "    new_row = []\n",
    "    features = row['features']\n",
    "    for i in range(len(features)): # i is column index\n",
    "        feature_list = []\n",
    "        feature_list.append(features[i])\n",
    "        new_row.append((i, feature_list))\n",
    "\n",
    "    return new_row\n",
    "\n",
    "# to be called with reduce by key\n",
    "# returns reassembled vectors based on column index\n",
    "def columnReduce(x, y): \n",
    "    if len(x) > len(y):\n",
    "        for item in y:\n",
    "            x.append(item)\n",
    "        return x\n",
    "    else:\n",
    "        for item in x:\n",
    "            y.append(item)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Returns summary stats for columns of original data\n",
    "def centerColumnMap(column):\n",
    "    column_index = column[0] # get key from previous mapReduce\n",
    "    mean = np.mean(column[1]) # get features\n",
    "    std = np.std(column[1], dtype=np.float64)\n",
    "\n",
    "    return (column_index, (mean, std))\n",
    "\n",
    "# Returns: centered, UNLABELED data\n",
    "# NOTE: cannot return labled data otherwise np.svd does not work\n",
    "def centerMap(row, column_data_dict):\n",
    "    label = row['label']\n",
    "    features = row['features']\n",
    "\n",
    "    centered_features = []\n",
    "    for i in range(len(features)): # Center the data\n",
    "        mean = column_data_dict[i][0]\n",
    "        std = column_data_dict[i][1]\n",
    "        if std != 0:\n",
    "            centered_features.append((features[i] - mean) / std)\n",
    "        else:\n",
    "            centered_features.append(features[i] - mean)\n",
    "\n",
    "    return centered_features\n",
    "\n",
    "def centerWithLabelsMap(row, column_data_dict):\n",
    "    label = row['label']\n",
    "    features = row['features']\n",
    "\n",
    "    centered_features = []\n",
    "    for i in range(len(features)): # Center the data\n",
    "        mean = column_data_dict[i][0]\n",
    "        std = column_data_dict[i][1]\n",
    "        if std != 0:\n",
    "            centered_features.append((features[i] - mean) / std)\n",
    "        else:\n",
    "            centered_features.append(features[i] - mean)\n",
    "\n",
    "    return (label, centered_features)\n",
    "\n",
    "### Part 2 functions\n",
    "\n",
    "# num components - num. principal components to be in returned matrix\n",
    "# input is of shape d * d, where d is the number of columns in the data matrix X\n",
    "# output is of form k * d, where k is the number of principal components # TODO: Make sure this is correct\n",
    "def get_pc_matrix(num_components, eigenvectors):\n",
    "    return eigenvectors[:,:num_components]\n",
    "\n",
    "# input: principal component matrix, of shape k * d # TODO, make sure this is correct\n",
    "# output: hat matrix, of shape k * d\n",
    "def get_hat_matrix(pc_matrix):\n",
    "    #hat_matrix = np.matmul(np.linalg.inv(np.matmul(pc_matrix, pc_matrix.transpose())), pc_matrix)\n",
    "    pc_matrix_transpose = pc_matrix.transpose()\n",
    "    hat_matrix = np.matmul(np.linalg.inv(np.matmul(pc_matrix_transpose, pc_matrix)), pc_matrix_transpose)\n",
    "    return hat_matrix\n",
    "\n",
    "# input hat_matrix, of shape k * d\n",
    "# input point, of shape d * 1\n",
    "# returns: parameters of projection: of form k * 1\n",
    "def project_point(hat_matrix, point_t):\n",
    "    return np.matmul(hat_matrix, point_t)\n",
    "\n",
    "def stripLabelsMap(row):\n",
    "    return row['features']\n",
    "\n",
    "# input principal component matrix, of shape d * k\n",
    "# projection parameters from project_point, of shape k * 1 \n",
    "def transform_back_to_d(pc_t_matrix, proj_parameters):\n",
    "    return np.matmul(pc_t_matrix, proj_parameters)\n",
    "\n",
    "def projectionErrorMap(row, pc_matrix, hat_matrix):\n",
    "    proj_parameters = project_point(hat_matrix, np.transpose(row))\n",
    "    transformed_vector = transform_back_to_d(pc_matrix.transpose(), proj_parameters)\n",
    "\n",
    "    squared_residual_sum = 0.0\n",
    "    # calculate projection error\n",
    "    for i in range(len(row)):\n",
    "        difference = row[i] - transformed_vector[i]\n",
    "        squared_residual_sum += (difference * difference)\n",
    "\n",
    "    return squared_residual_sum\n",
    "\n",
    "def projectionErrorReduce(x, y):\n",
    "    return x + y\n",
    "\n",
    "def predictionLabelReduce(x, y): # To be called with reduceByKey\n",
    "    if len(x) > len(y):\n",
    "        for item in y:\n",
    "            x.append(item)\n",
    "        return x\n",
    "    else:\n",
    "        for item in x:\n",
    "            y.append(item)\n",
    "        return y\n",
    "\n",
    "def kMeansModelMap(row, clusters):\n",
    "    predicted_label = clusters.predict(row['features'])\n",
    "    actual_label_list = []\n",
    "    actual_label_list.append(row['label'])\n",
    "    return (predicted_label, actual_label_list)\n",
    "\n",
    "# Part 2/3 K-Means functions\n",
    "def getClusterLabelsDict(prediction_results):\n",
    "    cluster_labels_dict = {}\n",
    "    \n",
    "    for key, cluster_list in prediction_results: # TODO: can probably combine with for loop below\n",
    "        most_common_label = max(set(cluster_list), key=cluster_list.count) # get most common label\n",
    "        cluster_labels_dict[key] = most_common_label\n",
    "        \n",
    "    return cluster_labels_dict\n",
    "\n",
    "def visualizePredictionResults(cluster_labels_dict, prediction_results):\n",
    "    sample_count = 30 # num samples from each cluster\n",
    "    for key, cluster_list in prediction_results:\n",
    "        # sample from cluster, visualize data\n",
    "        index_set = set()\n",
    "        for i in range(sample_count):\n",
    "            rand_indx = random.randint(0, len(cluster_list) - 1)\n",
    "            while rand_indx in index_set:\n",
    "                rand_indx = random.randint(0, len(cluster_list) - 1)\n",
    "            index_set.add(rand_indx)\n",
    "\n",
    "        subset = []\n",
    "        majority_count = 0.0\n",
    "        subset_label = cluster_labels_dict[key] # TODO: the sampling here does not quite work\n",
    "        for index in index_set:\n",
    "            subset.append(cluster_list[index])\n",
    "            if int(cluster_list[index]) == int(subset_label):\n",
    "                majority_count += 1.0\n",
    "        key_str = \"Cluster num. \" + str(key)\n",
    "        subset_label_str = \" has primary label \" + str(subset_label)\n",
    "        subset_str = \" : Samples of actual labels from cluster\"\n",
    "        print key_str + subset_label_str + subset_str\n",
    "        print subset\n",
    "        print \"Percentage majority label: \" + str(float(majority_count) / float(sample_count))\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "# Part 3 functions\n",
    "def similarityMatrixMap(row, h): # NOTE: to be used with uncentered data\n",
    "    calc_list = []\n",
    "    for data_vector in row:\n",
    "        subtraction = np.subtract(data_vector[0], data_vector[1])\n",
    "        magnitude = np.linalg.norm(subtraction)\n",
    "\n",
    "        calc_list.append(math.exp((-1.0 * magnitude) / h))\n",
    "\n",
    "    return calc_list\n",
    "\n",
    "\n",
    "# NOTE: to be used with uncentered data\n",
    "def rowSumMap(row):\n",
    "    return 1.0 / math.sqrt(np.sum(row['features']))\n",
    "\n",
    "def labelMap(row):\n",
    "    return row['label']\n",
    "\n",
    "# Part 4 functions\n",
    "def labeledPointMap(row):\n",
    "    label = row['label']\n",
    "    return LabeledPoint(row['label'], row['features'])\n",
    "\n",
    "def delabelLabeledPointMap(row):\n",
    "    return (row.label, row.features)\n",
    "\n",
    "def errorRateMap(row):\n",
    "    if int(row[0]) == row[1]:\n",
    "        return (1, 1)\n",
    "    else:\n",
    "        return (0, 1)\n",
    "def errorRateReduce(x, y):\n",
    "    return (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "def getFeaturesFromLabeledPointMap(row):\n",
    "    return row.features\n",
    "\n",
    "def getLabelFromLabeledPointMap(row):\n",
    "    return row.label\n",
    "\n",
    "def projectionMap(row, pc_matrix, hat_matrix):\n",
    "    proj_parameters = project_point(hat_matrix, np.transpose(row))\n",
    "    transformed_vector = transform_back_to_d(pc_matrix.transpose(), proj_parameters)\n",
    "    \n",
    "    return transformed_vector\n",
    "\n",
    "def unlabeledColumnFlatMap(row):\n",
    "    new_row = []\n",
    "    for i in range(len(row)): # i is column index\n",
    "        feature_list = []\n",
    "        feature_list.append(row[i])\n",
    "        new_row.append((i, feature_list))\n",
    "\n",
    "    return new_row\n",
    "\n",
    "def unlabeledCenterMap(row, column_data_dict):\n",
    "    \n",
    "    centered_features = []\n",
    "    for i in range(len(row)): # Center the data\n",
    "        mean = column_data_dict[i][0]\n",
    "        std = column_data_dict[i][1]\n",
    "        if std != 0:\n",
    "            centered_features.append((row[i] - mean) / std)\n",
    "        else:\n",
    "            centered_features.append(row[i] - mean)\n",
    "\n",
    "    return centered_features\n",
    "\n",
    "def labeledPointMap(row):\n",
    "    return LabeledPoint(row['label'], row['features'])\n",
    "\n",
    "def zipToLabeledPointMap(row):\n",
    "    return LabeledPoint(row[0], row[1])\n",
    "        \n",
    "############################################################\n",
    "\n",
    "#os.system('start-spark-slurm.sh&') # use & to put it into background\n",
    "\n",
    "# get ip address of this machine\n",
    "ip = os.popen('hostname -i').read().strip('\\n')\n",
    "print 'Spark Master web UI: http://{}:8080'.format(ip)\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('assn2').getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"mnist\").getOrCreate()\n",
    "# df = spark.read.json('/project/cmsc25025/mnist/data.json') # TODO IMPORTANT CHANGED FOR DEBUG.\n",
    "df = spark.read.json('/project/cmsc25025/mnist/data.json').repartition(NUM_PARTITIONS) # TODO IMPORTANT CHANGED FOR DEBUG.\n",
    "'''\n",
    "# part 1\n",
    "print \"Entering part 1\"\n",
    "# a)\n",
    "column_data_dict = {} # indexed by column index\n",
    "column_data = df.rdd.flatMap(columnFlatMap).reduceByKey(columnReduce).map(centerColumnMap).collect()\n",
    "for key, value in column_data:\n",
    "    column_data_dict[key] = value\n",
    "\n",
    "centered_data_rdd = df.rdd.map(lambda row: centerMap(row, column_data_dict))\n",
    "centered_train, centered_dev, centered_test = centered_data_rdd.randomSplit([4, 1, 1])\n",
    "\n",
    "# # need data to be unlabeled for this part\n",
    "# Cov Matrix\n",
    "mat = RowMatrix(centered_data_rdd)\n",
    "cov_matrix = mat.computeCovariance().toArray()\n",
    "eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)\n",
    "idx = eigen_values.argsort()[::-1]   \n",
    "eigen_values = eigen_values[idx]\n",
    "eigen_vectors = eigen_vectors[:,idx]\n",
    "\n",
    "components_to_visualize = 10\n",
    "visualization_subset = []\n",
    "for i in range(0, components_to_visualize):\n",
    "    pc = eigen_vectors[:,i]\n",
    "    visualization_subset.append(pc)\n",
    "\n",
    "# VISUALIZATION\n",
    "visualize(visualization_subset, title = \"Top 10 principal components\")\n",
    "\n",
    "#b)\n",
    "y_vector = []\n",
    "\n",
    "for var in eigen_values:\n",
    "    y_vector.append(var) \n",
    "\n",
    "x_vector = range(1, len(y_vector) + 1)  \n",
    "plt.clf()\n",
    "pylab.figure()\n",
    "plt.title(\"variance for each principal component\")\n",
    "plt.plot(x_vector, y_vector)\n",
    "plt.show()\n",
    "\n",
    "#c)\n",
    "components_to_test = 100 # test first x principal components\n",
    "pc_matrix_arr = []\n",
    "for i in range(10, components_to_test + 1, 10):\n",
    "    pc_matrix = get_pc_matrix(i, eigen_vectors)\n",
    "    pc_matrix_arr.append(pc_matrix)\n",
    "\n",
    "# VISUALIZATION\n",
    "num_to_sample = 10\n",
    "sample = centered_test.takeSample(False, num_to_sample, seed=0)\n",
    "point_arr = []\n",
    "for row in sample:\n",
    "    point_arr.append(np.transpose(row))\n",
    "\n",
    "store = []\n",
    "for point_t in point_arr:\n",
    "    substore = []\n",
    "    num_components = 1\n",
    "    sub_title = \"Num. principal components: \" + str(num_components)\n",
    "    for pc_matrix in pc_matrix_arr:\n",
    "        hat_matrix = get_hat_matrix(pc_matrix)\n",
    "        proj_parameters = project_point(hat_matrix, point_t)\n",
    "        substore.append(transform_back_to_d(pc_matrix, proj_parameters))\n",
    "    store.append((substore, sub_title))\n",
    "    num_components += 1\n",
    "\n",
    "# visualize the elements in store, which are points -> [projections onto first k prin. components]\n",
    "for substore, sub_title in store:\n",
    "    visualize(substore, sub_title)\n",
    "\n",
    "\n",
    "#d)\n",
    "results_dict = {}\n",
    "for i in range(10, components_to_test + 1, 10):\n",
    "    index = (i - 1) / 10\n",
    "    print \"Getting results for component \" + str(i)\n",
    "    hat_matrix = get_hat_matrix(pc_matrix_arr[index])\n",
    "    results_dict[index] = centered_test.map(lambda row:\\\n",
    "                                        projectionErrorMap(row, pc_matrix_arr[index].transpose(), hat_matrix)).\\\n",
    "                                        reduce(projectionErrorReduce)\n",
    "\n",
    "# VISUALIZATION\n",
    "x_vector = np.arange(10, components_to_test + 1, 10)\n",
    "y_vector = []\n",
    "for i in range(10, components_to_test + 1, 10):\n",
    "    index = (i - 1) / 10\n",
    "    y_vector.append(results_dict[index])\n",
    "\n",
    "plt.clf()\n",
    "pylab.figure() \n",
    "plt.title(\"Projection Error per Num of Principal Components\")\n",
    "plt.plot(x_vector, y_vector)\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "### Part 2 ###\n",
    "print \"Entering part 2\"\n",
    "#centered_data_rdd = df.map(lambda row: centerWithLabelsMap(row, column_data_dict)) TODO: Maybe use this?\n",
    "\n",
    "#clusters = KMeans.train(centered_data_rdd, 10, maxIterations=50, runs=10, initializationMode=\"random\")\n",
    "#predicted_rdd = clusters.predict(centered_data_rdd)\n",
    "p2_train = df\n",
    "\n",
    "label_list = p2_train.rdd.map(labelMap).collect()\n",
    "\n",
    "data_rdd = p2_train.rdd.map(stripLabelsMap)\n",
    "clusters = KMeans.train(data_rdd, 10, maxIterations=50, runs=10, initializationMode=\"random\")\n",
    "    \n",
    "prediction_results = p2_train.rdd.map(lambda row: kMeansModelMap(row, clusters)).reduceByKey(predictionLabelReduce).collect()\n",
    "cluster_labels_dict = getClusterLabelsDict(prediction_results) # maps cluster index to most common label in that cluster\n",
    "visualizePredictionResults(cluster_labels_dict, prediction_results)\n",
    "\n",
    "### Part 3 ###\n",
    "print \"Entering part 3\"\n",
    "h = 5.0\n",
    "# From Piazza post only using 10K images\n",
    "p3_subset, p3_test = df.rdd.randomSplit([1,99])\n",
    "\n",
    "# possible to filter duplicate cross products\n",
    "similarity_matrix = p3_subset.cartesian(p3_subset).flatMap(lambda row: similarityMatrixMap(row, h))\n",
    "\n",
    "dim = similarity_matrix.count()\n",
    "# http://stackoverflow.com/questions/42118005/create-an-identity-matrix-of-densevectors-as-a-spark-dataframe\n",
    "identity_matrix = np.identity(dim)\n",
    "diag_sim_matrix = np.diag(similarity_matrix.map(rowSumMap).collect())\n",
    "print(diag_sim_matrix[0:10,0:10])\n",
    "sym_laplacian = np.subtract(identity_matrix,\n",
    "                            np.matmul(np.matmul(diag_sim_matrix, similarity_matrix), diag_sim_matrix))\n",
    "\n",
    "highest_eigenval = sp.eigh(sym_laplacian, eigs_only=True, eigs=(0,0))[0] #changed to scipy eig\n",
    "\n",
    "scalar_diag_matrix = np.multiply(highest_eigenval, identity_matrix)\n",
    "num_vectors = range(2, 3 + 1, 1)\n",
    "\n",
    "for i in num_vectors:\n",
    "    pca = PCA(k=i)\n",
    "    pca_model = pca.fit(np.subtract(scalar_diag_matrix, sym_laplacian))\n",
    "\n",
    "    projected_data_rdd = pca_model.transform(p3_subset) # CHECK: make sure this works on labeled data\n",
    "    projected_cluster_model = KMeans.train(projected_data_rdd, 10, maxIterations=50, runs=10, initializationMode=\"random\")\n",
    "    # Check this\n",
    "    prediction_results = projected_cluster_model.predict(projected_data_rdd)\n",
    "\n",
    "    print \"Visualizing results for \" + i + \" eigenvectors\"\n",
    "    visualizePredictionResults(cluster_labels_dict, prediction_results)\n",
    "\n",
    "\n",
    "### Part 4 ###\n",
    "print \"Entering Part 4\"\n",
    "\n",
    "# part a\n",
    "labeledPointRDD = df.map(labeledPointMap)\n",
    "labeled_train, labeled_dev, labeled_test = labeledPointRDD.randomSplit([4, 1, 1])\n",
    "\n",
    "num_iterations = 50\n",
    "num_digits = 10 # 10 digits\n",
    "model = LogisticRegressionWithLBFGS.train(labeled_train, iterations=num_iterations, numClasses=num_digits)\n",
    "\n",
    "log_reg_results = labeled_test.map(lambda point: (point.label, model.predict(point.features))).map(errorRateMap).reduce(errorRateReduce)\n",
    "error_rate = 1.0 - (float(log_reg_results[0]) / float(log_reg_results[1]))\n",
    "print \"Error rate for full features: \" + str(error_rate)\n",
    "\n",
    "# part b\n",
    "\n",
    "results_dict = {}\n",
    "k_range = range(1, 50, 5)\n",
    "for k in range(k_range):\n",
    "    pca_model = PCA(k).fit(labeled_train) # Check: make sure this works\n",
    "    projected_train_rdd = pca_model.transform(labeled_rdd)\n",
    "\n",
    "    num_iterations = 50\n",
    "    num_digits = 10 # 10 digits\n",
    "    model = LogisticRegressionWithLBFGS.train(projected_train_rdd, iterations=num_iterations, numClasses=num_digits)\n",
    "    log_reg_results = labeled_test.map(lambda point: (point.label, model.predict(point.features))).\\\n",
    "                                    map(errorRateMap).reduce(errorRateReduce)\n",
    "    error_rate = 1.0 - (float(log_reg_results[0]) / float(log_reg_results[1]))\n",
    "    results_dict[k] = error_rate\n",
    "    print \"Error rate for \" + str(k) + \": \" + str(error_rate)\n",
    "\n",
    "# part c:\n",
    "lowest_pc_error = 1.1\n",
    "lowest_pc_key = 0.0\n",
    "for key, value in results_dict.items():\n",
    "    if value < lowest_pc_error:\n",
    "        lowest_pc_error = value\n",
    "        lowest_pc_key = key\n",
    "\n",
    "num_iterations = 50\n",
    "num_digits = 10 # 10 digits\n",
    "\n",
    "train_pca_model = PCA(lowest_pc_key).fit(labeled_train) # Check: make sure this works\n",
    "projected_train_rdd = pca_model.transform(labeled_rdd)\n",
    "train_model = LogisticRegressionWithLBFGS.train(projected_train_rdd, iterations=num_iterations, numClasses=num_digits)\n",
    "train_log_reg_results = labeled_test.map(lambda point: (point.label, train_model.predict(point.features))).\\\n",
    "                                    map(errorRateMap).reduce(errorRateReduce)\n",
    "train_error_rate = 1.0 - (float(train_log_reg_results[0]) / float(train_log_reg_results[1]))\n",
    "print \"Error rate for model trained on training data\" + str(lowest_pc_key) + \": \" + str(error_rate)\n",
    "\n",
    "dev_pca_model = PCA(lowest_pc_key).fit(labeled_dev) # Check: make sure this works\n",
    "projected_dev_rdd = dev_pca_model.transform(labeled_dev)\n",
    "dev_model = LogisticRegressionWithLBFGS.train(projected_dev_rdd, iterations=num_iterations, numClasses=num_digits)\n",
    "\n",
    "dev_log_reg_results = labeled_test.map(lambda point: (point.label, dev_model.predict(point.features))).\\\n",
    "                                    map(errorRateMap).reduce(errorRateReduce)\n",
    "test_error_rate = 1.0 - (float(test_log_reg_results[0]) / float(test_log_reg_results[1]))\n",
    "print \"Error rate for model trained on testing data\" + str(lowest_pc_key) + \": \" + str(error_rate)\n",
    "\n",
    "spark.stop()\n",
    "os.system('stop-spark-slurm.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
